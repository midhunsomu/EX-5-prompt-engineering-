# EXP 5: Comparative Analysis of Naïve Prompting versus Basic Prompting Using ChatGPT Across Various Test Scenarios
# Aim:
To test how ChatGPT responds to naïve prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios, analyzing the quality, accuracy, and depth of the generated responses.

# Algorithm: 
## 1. Define Prompt Types:

* Naïve Prompts are vague and lack direction, e.g., “Tell me something.”

* Basic Prompts are specific, structured, and instructive, e.g., “Write a three-sentence summary of Newton’s laws for high school students.”

## 2. Select Test Scenarios:

* Creative storytelling

* Factual question answering

* Concept summarization

* Giving practical advice

* Technical explanation

## 3. Create Prompt Pairs:

* For each scenario, formulate both a naïve and a basic version targeting the same goal.

## 4. Run and Record Outputs:

* Input both prompts into ChatGPT and record the respective responses.

## 5. Analyze and Compare:

* Evaluate each pair based on:

* Quality (fluency, engagement)

* Accuracy (factual correctness)

* Depth (level of insight or elaboration)

## Observations and Analysis:
1. In the creative storytelling task, the naïve prompt like “Tell me a story” generated a simple and generic narrative. However, the basic prompt “Write a short story about a rabbit who overcomes its fear of the dark” led to a well-structured and meaningful story with a moral.

2. When answering a factual question, the prompt “What is gravity?” resulted in a textbook-style response, while “Explain gravity to a 10-year-old using simple examples” produced a clearer and more relatable answer that matched the intended audience.

3. For summarizing a concept, the naïve prompt “Summarize AI” led to an overly general and unfocused summary. In contrast, “Summarize the impact of AI on healthcare in simple terms” yielded a concise and relevant explanation targeted to a specific domain.

4. In the advice-giving scenario, a naïve request like “Give me life advice” triggered vague, philosophical content. A basic prompt such as “Give three practical tips for a college student managing studies and a part-time job” produced actionable and focused advice.

5. Lastly, in the technical explanation task, a naïve “Explain Python” prompt gave a list of features. But a basic prompt like “Explain Python to a beginner with no programming experience” delivered a friendly, step-by-step introduction.

## Findings:
 * Basic prompts consistently resulted in higher quality, more relevant, and deeper responses.

 * Naïve prompts only worked well when no clear outcome or structure was required.

 * Tasks involving teaching, storytelling, or complex reasoning significantly benefited from refined prompting.

 * This experiment shows that prompt clarity and specificity are crucial for optimizing ChatGPT’s capabilities.

# OUTPUT:
All prompt-response pairs were tested and documented. The differences between naïve and basic prompting were clearly observed across all tasks.

# RESULT: 
The prompt for the above said problem executed successfully
